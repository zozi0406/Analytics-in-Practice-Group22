---
title: "Analysis"
author: "Zoltan Aldott"
date: "11/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(tidyverse)
library(caTools)
library(caret)
library(DataExplorer)
library(gbm)
library(mlbench)
library(smotefamily)
library(themis)
library(MLmetrics)
library(pROC)
library(MLeval)
library(rpart)
library(glmnet)
```


```{r import}

#Import data
credits <- read_csv("assignment_data.csv")


#Inspect the structure of the data
credits %>% str

#Drop ID column
credits$ID<-NULL

#Based on readme.txt data dictionary, keep track of all numeric columns.
numericcols <- c("LIMIT","AGE","BILL1","BILL2","BILL3","BILL4","BILL5","BILL6","PYAMT1","PYAMT2","PYAMT3","PYAMT4","PYAMT5","PYAMT6","CREDITCRD","PY1","PY2","PY3","PY4","PY5","PY6","YEARSINADD")

#Also find a list of numeric columns for later convenience.
non_numericcols<-names(credits) %>% setdiff(numericcols)

#Convert non-numeric columns to factors.
credits[,non_numericcols] <- lapply(credits[,non_numericcols],factor)

#Set levels for ordered factors.
credits$SATISFACTION<-credits$SATISFACTION %>% ordered(levels=c(0,1,2))
credits$FREQTRANSACTION<-credits$FREQTRANSACTION %>% ordered(levels=c(0,1))

#Other ordered factors like YEARSINADD and PY# are treated as numeric to reduce model sizes

#Revalue class to allow easier interpretability
credits$CLASS <- credits$CLASS %>% revalue(c("0"="NO_DEFAULT","1"="DEFAULT"))
credits$CLASS <- factor(credits$CLASS, levels=rev(levels(credits$CLASS)))





#Drop missing values, as they cannot be reliably replaced by imputation and they constitute less than 1% of the data.
DataExplorer::plot_missing(credits,missing_only = T,title="Missing values in the dataset",group=list("Missing"=0.05))
ggsave("missing_plot.jpg",dpi=500)

complete <- complete.cases(credits)
credits<-credits[complete,]

#Plot discrete features
plot_bar(credits)

#Group Education=0,4,5,6 to reduce model sizes, these were tested to have no significant positive impact on predictive power.
credits$EDUCATION<-ifelse(as.numeric(as.character(credits$EDUCATION)) %in% c(4,5,6),0,as.numeric(as.character(credits$EDUCATION))) %>% factor

#Plot continuous features
plot_histogram(credits)
#PY1-6 negative values will be dealt with below, otherwise all variables' ranges make sense for the values they represent. 
#Even though there are outliers in the BILL and PYAMT variables, these are not removed, as they may provide additional predictive information.

#To confirm the ranges of the variables:
credits %>% summary
#The ranges do not fall outside of heuristically realistic ranges even for the variables with outliers.

# No variation in cm_hist, so dropped
credits$CM_HIST<-NULL

#Age is retained instead of age category
credits$AGE_CTG<-NULL

#Remove duplicates
credits <- credits %>% distinct()

#Identify class imbalance problem
ggplot(credits,aes(y=CLASS,x=..count..))+geom_bar(fill="coral2")+labs(title="Distribution of target class",x="Count",y="Class label") + theme_bw()



```


```{r baseline_datasets}

#The code below creates a rich and a sparse definition of the PY1-6 variables whether to preserve 
#or to drop the information (No transactions and paid in full) stored in its negative values.


data_defs<-list()

#Class is not included in data definitions to avoid issues with contrast encoding below. It is re-added later
data_defs$sparse<-credits %>% select(-CLASS)

labels<-credits$CLASS

for (i in 1:6){
  data_defs$sparse[[paste0("PY",i)]]<-ifelse(data_defs$sparse[[paste0("PY",i)]]<0,0,data_defs$sparse[[paste0("PY",i)]])
}

data_defs$rich<-credits %>% select(-CLASS)

for (i in 1:6){
  data_defs$rich[[paste0("paidfull",i)]]<-ifelse(data_defs$rich[[paste0("PY",i)]]==-1,1,0)
  data_defs$rich[[paste0("notransaction",i)]]<-ifelse(data_defs$rich[[paste0("PY",i)]]==-2,1,0)
  data_defs$rich[[paste0("PY",i)]]<-ifelse(data_defs$rich[[paste0("PY",i)]]<0,0,data_defs$rich[[paste0("PY",i)]])
}

#Scale numeric columns
data_defs$sparse[,numericcols]<-lapply(data_defs$sparse[,numericcols],scale) %>% lapply(`[`,c(1:nrow(data_defs$sparse)))
data_defs$rich[,numericcols]<-lapply(data_defs$rich[,numericcols],scale) %>% lapply(`[`,c(1:nrow(data_defs$rich)))

#Apply default contrast encoding to non-numeric variables
data_defs$sparse<-model.matrix(~.+1,data_defs$sparse)[,-1] %>% as_tibble
data_defs$rich<-model.matrix(~.+1,data_defs$rich)[,-1] %>% as_tibble
```





```{r sample_splitting_and_sampling}
#REWRITE

#Set seed for reproducibility
set.seed(100)
#Create training and test sample splits
training<-sample.split(credits$CLASS,SplitRatio = 0.8)
test<-!training
training_labels<-labels[training]
test_labels<-labels[!training]


train_sets<-list()
train_sets$sparse<-data_defs$sparse[training,]
train_sets$rich<-data_defs$rich[training,]

#Add class column
train_sets$sparse<-train_sets$sparse %>% add_column(class=training_labels)
train_sets$rich<-train_sets$rich %>% add_column(class=training_labels)

#Define test sets for the two data definitions
test_sparse<-data_defs$sparse[!training,] %>% add_column(class=test_labels)
test_rich<-data_defs$rich[!training,] %>% add_column(class=test_labels)

#Clear up RAM used for redundant dataframes
rm(credits,data_defs)
```


```{r baseline_modeling, warning=FALSE}
#Modelling is done via caret's train() framework.
#Define settings for cross-validation and model evaluation
tc_normal<-trainControl(method = "cv" ,number = 5 , classProbs = TRUE,summaryFunction = prSummary, savePredictions = TRUE)
tc_smote<-trainControl(method = "cv" ,number = 5 , classProbs = TRUE,summaryFunction = prSummary,sampling="smote", savePredictions = TRUE)
tc_over<-trainControl(method = "cv" ,number = 5 , classProbs = TRUE,summaryFunction = prSummary,sampling="up", savePredictions = TRUE)
tc_under<-trainControl(method = "cv" ,number = 5, classProbs = TRUE,summaryFunction = prSummary,sampling="down", savePredictions = TRUE)

#Define function for cross-validated model evaluation
evaluate_model <- function(formula=class~.,data,method,train_control=tc_normal,tuneLength=1, grid=NULL, ...) {
  #Set seed again to ensure consistent data-splits for cross-validation
  set.seed(100)
  fit<-train(form=formula,data=data,method=method,trControl=train_control,metric="F",maximize=T,tuneGrid=grid,tuneLength=tuneLength,...)
  return(fit)
}

#Define function for evaluation on test data.
testeval <- function(data,fit){
  preds<-predict(fit,data)
  cm<-confusionMatrix(preds,data$class,mode="prec_recall")
  return(cm)
}

#Given its consistent track record across the literature and its non-reliance on hyperparameters, the logistic regression is used to evaluate, which dataset to use for modeling


#Note, when oversampling, the cross-validation scores will not be comparable across 


baseline=list()
baseline$sparse<-evaluate_model(data=train_sets$sparse, method="glm")
baseline$sparse_smote<-evaluate_model(data=train_sets$sparse, method="glm",train_control=tc_smote)
baseline$sparse_over<-evaluate_model(data=train_sets$sparse, method="glm",train_control=tc_over)
baseline$sparse_under<-evaluate_model(data=train_sets$sparse, method="glm",train_control=tc_under)
baseline$rich<-evaluate_model(data=train_sets$rich, method="glm")
baseline$rich_smote<-evaluate_model(data=train_sets$rich, method="glm",train_control=tc_smote)
baseline$rich_over<-evaluate_model(data=train_sets$rich, method="glm",train_control=tc_over)
baseline$rich_under<-evaluate_model(data=train_sets$rich, method="glm",train_control=tc_under)

baseline_res <- data.frame()
for (i in names(baseline)) {
    baseline_res<-baseline[[i]]$results[1,] %>% add_column(dataset=i) %>%  bind_rows(baseline_res)
}

#Note, the AUC included here refers to the area under the PRG (Precision-Recall gain) curve.
#The main metric used here is the F score which attempts to balance Precision and Recall.

ggplot(baseline_res,aes(x=F,y=reorder(dataset,F))) +geom_col(fill="coral2")+theme_bw()+labs(y="Dataset & Sampling",x="F1 score",title="Logistic regression F1 scores datasets and sampling methods")

#The rich_over sample is used, which uses caret's built-in random over-sampling algorithm.
#Although the selection of a different cutoff may yield different results, 
#due to limited computing capacity, at this stage optimising the threshold is not possible.


training_set<-train_sets$rich
test_set<-test_rich
train_control<-tc_over


```

```{r model_choice, warning=FALSE}

model_comparison<-list()

#The key common features of the algorithms compared below include scalability and speed.
#Although other classification algorithms may also do well, their computational costs tend to scale significantly with observations and variables.
#As heuristics do not allow to drop any variables in this case, only such methods are explored that scale efficiently and are not prone to overfit
#when including a high number of features.


#Logistic regression has no hyperparameters
model_comparison$Logistic_regression<-evaluate_model(data=training_set, method="glm",train_control = train_control)
#Regularized (L1 and L2) logistic regression
model_comparison$Penalised_Logistic_Regression<-evaluate_model(data=training_set, method="glmnet",train_control = train_control)
#Gradient-boosting machine
model_comparison$GBM<-evaluate_model(data=training_set, method="gbm",train_control = train_control,verbose=F)
#XGBoost
model_comparison$XGBoost<-evaluate_model(data=training_set, method="xgbTree",train_control = train_control)
#Decision tree
model_comparison$Tree<-evaluate_model(data=training_set, method="rpart",train_control = train_control)

model_res <- data.frame()
for (i in names(model_comparison)) {
    model_res<-model_comparison[[i]]$results[1,] %>% add_column(model=i) %>% select(model,F,Recall,Precision) %>% bind_rows(model_res)
}

#Note, the AUC included here refers to the area under the PRG (Precision-Recall gain) curve.
#The main metric used here is the F score which attempts to balance Precision and Recall.

#XGBoost, a variation of the gradient boosting algorithm seems to be performing the best with default hyperparameters. This will be tuned further to determine the final model.
ggplot(model_res,aes(x=F,y=reorder(model,F))) +geom_col(fill="coral2")+theme_bw()+labs(y="Model",x="F1 score",title="F1 scores across algorithms with default hyperparameters")

```


```{r Hyperparameter_tuning_and_final_model}

#The recent update to XGboost introduced a deprecation warning, which is silenced below. The rest of the functionality remains as intended.
#The below command creates a grid of n^5 models and chooses the one with the optimal Cross-validated F1_score.

#The following grid search is a reduced version of a longer tuning process to indicate the methodology.
if (!file.exists("xgb_tuning.rds")){
  capture.output(xgb_tuning<-evaluate_model(data=training_set,method="xgbTree",train_control=train_control,grid=expand.grid(
    "eta"=c(0.1,0.05),
    "min_child_weight"=c(2,3),
    "max_depth"=c(2,3),
    "nrounds"=c(50,100),
    "gamma"=c(0.1,0.2),
    "subsample"=c(0.9,1),
    "colsample_bytree"=c(0.75,0.85))),file=nullfile())
  saveRDS(xgb_tuning,"xgb_tuning.rds")
} else {
  xgb_tuning <- readRDS("xgb_tuning.rds")
}

#Single threading specified to eliminate discrepancies due to parallel processing.
capture.output(final_xgb<-evaluate_model(data=training_set,method="xgbTree",train_control=train_control,grid=expand.grid(
    "eta"=c(0.05),
    "min_child_weight"=c(2),
    "max_depth"=c(3),
    "nrounds"=c(100),
    "gamma"=c(0.2),
    "subsample"=c(1),
    "colsample_bytree"=c(0.75)),nthread=1),file=nullfile())



#Due to the nature of gradient boosting models and xgboost's parallel processing, the exact result varies slightly from the one achieved above, despite setting a random seed before training. Nonetheless, the result is not significantly different.

#The xgb_tuning tuning object automatically selects the final model as the one with the highest F1_score, which can be used to evaluate the model performance on the test set.
(test1<-testeval(test_set,final_xgb))

#To ensure that the hyperparameter tuning has actually yielded increased performance, models from previous parts of the process are also evaluated.
(test2<-testeval(test_set,model_comparison$XGBoost))
(test3<-testeval(test_set,baseline$rich_over))


test_F1s<-data.frame(model=c("Final model","Default XGBoost","Logistic Regression"),F=c(test1$byClass["F1"],test2$byClass["F1"],test3$byClass["F1"]))

#The model achieves a similarly high F1 score on the test-set which suggests the model is not overfitting on the cross-validation splits.
ggplot(test_F1s,aes(x=F,y=reorder(model,F))) +geom_col(fill="coral2")+theme_bw()+labs(y="Model",x="F1 score",title="F1 scores across algorithms with default hyperparameters")



#Cumulative gains chart. Caret refers to it as lift.
test_predictions<-predict(final_xgb,test_set, type="prob")[,"DEFAULT"]
pred_and_true<-data.frame(class=test_labels,predictions=test_predictions)
roc_vals<-roc(class~predictions,pred_and_true,levels=c("NO_DEFAULT","DEFAULT"))
ROC_out<-data.frame(sensitivity=roc_vals$sensitivities,specificity=roc_vals$specificities,thresholds=roc_vals$thresholds)
write.csv(ROC_out,"roc.csv")
write.csv(test_predictions,"test_preds.csv")


lift_vals<-lift(class~predictions,pred_and_true)
ggplot(lift_vals,values=60) + theme_bw()

```


```{r Produce_charts}



```